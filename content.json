{"meta":{"title":"Li Yao","subtitle":"","description":"","author":"Li Yao","url":"https://www.yaobio.com","root":"/"},"pages":[{"title":"Posts","date":"2020-04-19T16:27:02.000Z","updated":"2025-04-23T04:03:22.961Z","comments":true,"path":"posts/index.html","permalink":"https://www.yaobio.com/posts/index.html","excerpt":"","text":""},{"title":"Publications","date":"2020-04-19T17:11:09.000Z","updated":"2025-04-23T04:03:22.961Z","comments":true,"path":"publications/index.html","permalink":"https://www.yaobio.com/publications/index.html","excerpt":"","text":""}],"posts":[{"title":"High-resolution reconstruction of cell-type specific transcriptional regulatory processes from bulk sequencing samples","slug":"publications/2025-deepdetails","date":"2025-04-06T14:42:06.000Z","updated":"2025-04-23T04:03:22.948Z","comments":true,"path":"/publication/high-resolution-reconstruction-of-cell-type-specific-transcriptional-regulatory-processes-from-bulk-sequencing-samples/","link":"","permalink":"https://www.yaobio.com/publication/high-resolution-reconstruction-of-cell-type-specific-transcriptional-regulatory-processes-from-bulk-sequencing-samples/","excerpt":"","text":"","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"Comprehensive Evaluation of Diverse Massively Parallel Reporter Assays to Functionally Characterize Human Enhancers Genome-wide","slug":"publications/2025-STARRseq-benchmark","date":"2025-03-27T18:41:29.000Z","updated":"2025-04-23T04:03:22.948Z","comments":true,"path":"/publication/comprehensive-evaluation-of-diverse-massively-parallel-reporter-assays-to-functionally-characterize-human-enhancers-genome-wide/","link":"","permalink":"https://www.yaobio.com/publication/comprehensive-evaluation-of-diverse-massively-parallel-reporter-assays-to-functionally-characterize-human-enhancers-genome-wide/","excerpt":"","text":"","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"Simultaneous measurement of intrinsic promoter and enhancer potential reveals principles of functional duality and regulatory reciprocity","slug":"publications/2025-QUASARR-seq","date":"2025-03-15T18:41:29.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/simultaneous-measurement-of-intrinsic-promoter-and-enhancer-potential-reveals-principles-of-functional-duality-and-regulatory-reciprocity/","link":"","permalink":"https://www.yaobio.com/publication/simultaneous-measurement-of-intrinsic-promoter-and-enhancer-potential-reveals-principles-of-functional-duality-and-regulatory-reciprocity/","excerpt":"","text":"","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"Loss of Kmt2c or Kmt2d primes urothelium for tumorigenesis and redistributes KMT2A-menin to bivalent promoters","slug":"publications/2024-kmt2cd","date":"2025-01-13T18:41:29.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/loss-of-kmt2c-or-kmt2d-primes-urothelium-for-tumorigenesis-and-redistributes-kmt2a-menin-to-bivalent-promoters/","link":"","permalink":"https://www.yaobio.com/publication/loss-of-kmt2c-or-kmt2d-primes-urothelium-for-tumorigenesis-and-redistributes-kmt2a-menin-to-bivalent-promoters/","excerpt":"Members of the KMT2C&#x2F;D-KDM6A complex are recurrently mutated in urothelial carcinoma and in histologically normal urothelium. Here, using genetically engineered mouse models, we demonstrate that Kmt2c&#x2F;d knockout in the urothelium led to impaired differentiation, augmented responses to growth and inflammatory stimuli and sensitization to oncogenic transformation by carcinogen and oncogenes. Mechanistically, KMT2D localized to active enhancers and CpG-poor promoters that preferentially regulate the urothelial lineage program and Kmt2c&#x2F;d knockout led to diminished H3K4me1, H3K27ac and nascent RNA transcription at these sites, which leads to impaired differentiation. Kmt2c&#x2F;d knockout further led to KMT2A-menin redistribution from KMT2D localized enhancers to CpG-high and bivalent promoters, resulting in derepression of signal-induced immediate early genes. Therapeutically, Kmt2c&#x2F;d knockout upregulated epidermal growth factor receptor signaling and conferred vulnerability to epidermal growth factor receptor inhibitors. Together, our data posit that functional loss of Kmt2c&#x2F;d licenses a molecular ‘field effect’ priming histologically normal urothelium for oncogenic transformation and presents therapeutic vulnerabilities.","text":"Abstract Members of the KMT2C&#x2F;D-KDM6A complex are recurrently mutated in urothelial carcinoma and in histologically normal urothelium. Here, using genetically engineered mouse models, we demonstrate that Kmt2c&#x2F;d knockout in the urothelium led to impaired differentiation, augmented responses to growth and inflammatory stimuli and sensitization to oncogenic transformation by carcinogen and oncogenes. Mechanistically, KMT2D localized to active enhancers and CpG-poor promoters that preferentially regulate the urothelial lineage program and Kmt2c&#x2F;d knockout led to diminished H3K4me1, H3K27ac and nascent RNA transcription at these sites, which leads to impaired differentiation. Kmt2c&#x2F;d knockout further led to KMT2A-menin redistribution from KMT2D localized enhancers to CpG-high and bivalent promoters, resulting in derepression of signal-induced immediate early genes. Therapeutically, Kmt2c&#x2F;d knockout upregulated epidermal growth factor receptor signaling and conferred vulnerability to epidermal growth factor receptor inhibitors. Together, our data posit that functional loss of Kmt2c&#x2F;d licenses a molecular ‘field effect’ priming histologically normal urothelium for oncogenic transformation and presents therapeutic vulnerabilities. Manuscript","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"Finding Needles in the Haystack: Strategies for Uncovering Noncoding Regulatory Variants","slug":"publications/2023-ncv-review","date":"2023-08-06T12:41:29.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/finding-needles-in-the-haystack-strategies-for-uncovering-noncoding-regulatory-variants/","link":"","permalink":"https://www.yaobio.com/publication/finding-needles-in-the-haystack-strategies-for-uncovering-noncoding-regulatory-variants/","excerpt":"Despite accumulating evidence implicating noncoding variants in human diseases, unraveling their functionality remains a significant challenge. Systematic annotations of the regulatory landscape and the growth of sequence variant data sets have fueled the development of tools and methods to identify causal noncoding variants and evaluate their regulatory effects. Here, we review the latest advances in the field and discuss potential future research avenues to gain a more in-depth understanding of noncoding regulatory variants.","text":"Abstract Despite accumulating evidence implicating noncoding variants in human diseases, unraveling their functionality remains a significant challenge. Systematic annotations of the regulatory landscape and the growth of sequence variant data sets have fueled the development of tools and methods to identify causal noncoding variants and evaluate their regulatory effects. Here, we review the latest advances in the field and discuss potential future research avenues to gain a more in-depth understanding of noncoding regulatory variants. Manuscript","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"Functional genomic assays to annotate enhancer-promoter interactions genome-wide","slug":"publications/2022-epi-review","date":"2022-08-26T14:08:29.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/functional-genomic-assays-to-annotate-enhancer-promoter-interactions-genome-wide/","link":"","permalink":"https://www.yaobio.com/publication/functional-genomic-assays-to-annotate-enhancer-promoter-interactions-genome-wide/","excerpt":"Enhancers are pivotal for regulating gene transcription that occurs at promoters. Identification of the interacting enhancer–promoter pairs and understanding the mechanisms behind how they interact and how enhancers modulate transcription can provide fundamental insight into gene regulatory networks. Recently, advances in high-throughput methods in three major areas—chromosome conformation capture assay, such as Hi-C to study basic chromatin architecture, ectopic reporter experiments such as self-transcribing active regulatory region sequencing (STARR-seq) to quantify promoter and enhancer activity, and endogenous perturbations such as clustered regularly interspaced short palindromic repeat interference (CRISPRi) to identify enhancer–promoter compatibility—have further our knowledge about transcription. In this review, we will discuss the major method developments and key findings from these assays.","text":"Abstract Enhancers are pivotal for regulating gene transcription that occurs at promoters. Identification of the interacting enhancer–promoter pairs and understanding the mechanisms behind how they interact and how enhancers modulate transcription can provide fundamental insight into gene regulatory networks. Recently, advances in high-throughput methods in three major areas—chromosome conformation capture assay, such as Hi-C to study basic chromatin architecture, ectopic reporter experiments such as self-transcribing active regulatory region sequencing (STARR-seq) to quantify promoter and enhancer activity, and endogenous perturbations such as clustered regularly interspaced short palindromic repeat interference (CRISPRi) to identify enhancer–promoter compatibility—have further our knowledge about transcription. In this review, we will discuss the major method developments and key findings from these assays. Accepted manuscript","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"Survey of the binding preferences of RNA-binding proteins to RNA editing events","slug":"publications/2022-rbp-preference","date":"2022-08-16T14:47:18.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/survey-of-the-binding-preferences-of-rna-binding-proteins-to-rna-editing-events/","link":"","permalink":"https://www.yaobio.com/publication/survey-of-the-binding-preferences-of-rna-binding-proteins-to-rna-editing-events/","excerpt":"Background: Adenosine-to-inosine (A-to-I) editing is an important RNA posttranscriptional process related to a multitude of cellular and molecular activities. However, systematic characterizations of whether and how the events of RNA editing are associated with the binding preferences of RNA sequences to RNA-binding proteins (RBPs) are still lacking. Results: With the RNA-seq and RBP eCLIP-seq datasets from the ENCODE project, we quantitatively survey the binding preferences of 150 RBPs to RNA editing events, followed by experimental validations. Such analyses of the RBP-associated RNA editing at nucleotide resolution and genome-wide scale shed light on the involvement of RBPs specifically in RNA editing-related processes, such as RNA splicing, RNA secondary structures, RNA decay, and other posttranscriptional processes. Conclusions: These results highlight the relevance of RNA editing in the functions of many RBPs and therefore serve as a resource for further characterization of the functional associations between various RNA editing events and RBPs.","text":"Abstract Background: Adenosine-to-inosine (A-to-I) editing is an important RNA posttranscriptional process related to a multitude of cellular and molecular activities. However, systematic characterizations of whether and how the events of RNA editing are associated with the binding preferences of RNA sequences to RNA-binding proteins (RBPs) are still lacking. Results: With the RNA-seq and RBP eCLIP-seq datasets from the ENCODE project, we quantitatively survey the binding preferences of 150 RBPs to RNA editing events, followed by experimental validations. Such analyses of the RBP-associated RNA editing at nucleotide resolution and genome-wide scale shed light on the involvement of RBPs specifically in RNA editing-related processes, such as RNA splicing, RNA secondary structures, RNA decay, and other posttranscriptional processes. Conclusions: These results highlight the relevance of RNA editing in the functions of many RBPs and therefore serve as a resource for further characterization of the functional associations between various RNA editing events and RBPs. Published manuscript","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"Capped nascent RNA sequencing reveals novel therapy-responsive enhancers in prostate cancer","slug":"publications/2022-prostate-procap","date":"2022-04-08T18:41:29.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/capped-nascent-rna-sequencing-reveals-novel-therapy-responsive-enhancers-in-prostate-cancer/","link":"","permalink":"https://www.yaobio.com/publication/capped-nascent-rna-sequencing-reveals-novel-therapy-responsive-enhancers-in-prostate-cancer/","excerpt":"","text":"","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"A comparison of experimental assays and analytical methods for genome-wide identification of active enhancers","slug":"publications/2022-pints","date":"2022-01-06T14:42:06.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/a-comparison-of-experimental-assays-and-analytical-methods-for-genome-wide-identification-of-active-enhancers/","link":"","permalink":"https://www.yaobio.com/publication/a-comparison-of-experimental-assays-and-analytical-methods-for-genome-wide-identification-of-active-enhancers/","excerpt":"Mounting evidence supports the idea that transcriptional patterns serve as more specific identifiers of active enhancers than histone marks; however, the optimal strategy to identify active enhancers both experimentally and computationally has not been determined. Here, we compared 13 genome-wide RNA sequencing (RNA-seq) assays in K562 cells and show that nuclear run-on followed by cap-selection assay (GRO&#x2F;PRO-cap) has advantages in enhancer RNA detection and active enhancer identification. We also introduce a tool, peak identifier for nascent transcript starts (PINTS), to identify active promoters and enhancers genome wide and pinpoint the precise location of 5′ transcription start sites. Finally, we compiled a comprehensive enhancer candidate compendium based on the detected enhancer RNA (eRNA) transcription start sites (TSSs) available in 120 cell and tissue types, which can be accessed at https://pints.yulab.org. With knowledge of the best available assays and pipelines, this large-scale annotation of candidate enhancers will pave the way for selection and characterization of their functions in a timeand labor-efficient manner.","text":"Abstract Mounting evidence supports the idea that transcriptional patterns serve as more specific identifiers of active enhancers than histone marks; however, the optimal strategy to identify active enhancers both experimentally and computationally has not been determined. Here, we compared 13 genome-wide RNA sequencing (RNA-seq) assays in K562 cells and show that nuclear run-on followed by cap-selection assay (GRO&#x2F;PRO-cap) has advantages in enhancer RNA detection and active enhancer identification. We also introduce a tool, peak identifier for nascent transcript starts (PINTS), to identify active promoters and enhancers genome wide and pinpoint the precise location of 5′ transcription start sites. Finally, we compiled a comprehensive enhancer candidate compendium based on the detected enhancer RNA (eRNA) transcription start sites (TSSs) available in 120 cell and tissue types, which can be accessed at https://pints.yulab.org. With knowledge of the best available assays and pipelines, this large-scale annotation of candidate enhancers will pave the way for selection and characterization of their functions in a timeand labor-efficient manner. Accepted manuscript","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"Case Study: Resolving the Mystery of a Full Linux Root Disk","slug":"posts/server-debug-df-du","date":"2021-09-04T00:47:40.000Z","updated":"2025-04-23T04:03:22.946Z","comments":true,"path":"/posts/case-study-resolving-the-mystery-of-a-full-linux-root-disk/","link":"","permalink":"https://www.yaobio.com/posts/case-study-resolving-the-mystery-of-a-full-linux-root-disk/","excerpt":"","text":"In the world of genomics research, every data point carries the potential to unlock the secrets of life itself. However, as genomics datasets grow in size and complexity, so do the challenges of managing the associated disk space. This is a story of a recent journey that began with a simple task: using the Integrative Genomics Viewer (IGV) to inspect many signal tracks. Little did I know that this journey would lead to a deep dive into the enigmatic realm of Linux file handles and the secrets they held. The Enigmatic MessageThe story unfolds in the midst of my genomics analysis. I was inspecting numerous signal tracks with IGV. My workflow consisted of working in batched samples—inspect, analyze, delete. As I diligently removed the tracks from IGV and deleted the corresponding files from the filesystem, all seemed well. That was until an unexpected message disrupted my rhythm: “Root disk full.” The Perplexing DiscrepancyThe message left me bewildered. How could the root disk be full when I had meticulously deleted the files associated with the inspected signal tracks? I first checked the situation with the df command and confirmed that the system still believed it was running out of space. 1df -h 1234Filesystem Size Used Avail Use% Mounted ontmpfs 50G 3.0M 50G 1% /run/dev/sda1 58G 58G 455M 100% /... Determined to solve this mystery, I turned to the du command, the venerable tool for disk usage analysis. Yet, to my astonishment, du reported a different story. It indicated that the disk had ample space, with only 24G being used. 1sudo du -h --max-depth=1 / 12345678.3G /usr5.4G /home7.1M /tmp4.9G /snap5.1G /var...24G . The Unyielding File HandlesThe perplexing discrepancy between the reported disk usage and the du output left me scratching my head. What could possibly be holding onto that disk space? I couldn’t help but wonder if I had missed something when running the du command. But no, I had executed the command on the entire root directory with all the privileges of an admin. So, what other mysterious forces could be at play? A crucial revelation came to light as I delved deeper into the mystery. There may be files that were still open despite being deleted. The question now was: how do I unearth these orphaned files and identify the processes holding them hostage? To answer this question, I turned to the ‘lsof’ command. Specifically, I used the following command to search for orphaned files and the processes that kept them open: 1lsof +aL1 / The output was illuminating. A single Java process, with ID 455114, was responsible for holding onto all the orphaned files, each contributing to the discrepancy in disk space usage. This was evident from the SIZE/OFF column, which displayed substantial figures. 123456COMMAND PID USER FD TYPE DEVICE SIZE/OFF NLINK NODE NAMEjava 455114 li 118r REG 8,1 624602003 0 1552486 /home/li/.local/share/Trash/expunged/751520316 (deleted)java 455114 li 119r REG 8,1 1180123471 0 1552300 /home/li/.local/share/Trash/expunged/2560126835 (deleted)java 455114 li 129r REG 8,1 225277230 0 1578319 /home/li/.local/share/Trash/expunged/2423833192 (deleted)...java 455114 li 221r REG 8,1 2846628790 0 1582420 /home/li/.local/share/Trash/expunged/2658608637 (deleted) The Pursuit of ResolutionArmed with this newfound knowledge, I was determined to catch the ghost occupying my disk space. The first step in this quest was to identify the program behind those tenacious file handles. I began my search by unmasking the process responsible for these file handles. To do this, I used the ps aux command with grep to filter the results based on the process ID (455114) I discovered earlier. The command looked like this: 1ps aux | grep 455114 The result was a revealing snapshot of the process: 1li 455114 0.3 0.4 22382052 2385040 pts/6 Sl+ Aug30 20:32 java -showversion --module-path=/home/li/IGV_Linux_2.16.2/lib -Xmx8g @/home/li/IGV_Linux_2.16.2/igv.args -Dapple.laf.useScreenMenuBar=true -Djava.net.preferIPv4Stack=true -Djava.net.useSystemProxies=true @/home/li/.igv/java_arguments --module=org.igv/org.broad.igv.ui.Main Ah, the culprit was none other than IGV itself! Despite removing the tracks from IGV and deleting the corresponding files from the filesystem, IGV had clung onto the file handles, refusing to release them. The pieces of the puzzle were finally coming together, and a solution was now within reach. Right after I terminated the IGV instance, a lot of disk space got released, and the problem was solved! 1df -h 1234Filesystem Size Used Avail Use% Mounted ontmpfs 50G 3.0M 50G 1% /run/dev/sda1 58G 18G 41G 31% /...","categories":[{"name":"posts","slug":"posts","permalink":"https://www.yaobio.com/posts/"}],"tags":[{"name":"technical_docs","slug":"technical-docs","permalink":"https://www.yaobio.com/tags/technical-docs/"},{"name":"debug","slug":"debug","permalink":"https://www.yaobio.com/tags/debug/"}]},{"title":"PINTS and PINTS web portal","slug":"tools/PINTS","date":"2021-03-07T23:44:11.000Z","updated":"2025-04-23T04:03:22.948Z","comments":true,"path":"/tools/pints-and-pints-web-portal/","link":"","permalink":"https://www.yaobio.com/tools/pints-and-pints-web-portal/","excerpt":"Peak Identifier for Nascent Transcript Starts (PINTS) is a software that helps locate potential regulatory elements (promoters and enhancers) genome-wide from nascent transcript sequencing data, like GRO&#x2F;PRO-cap libraries, which are specifically enriched for signals from transcription start sites (TSSs). PINTS web portal is a comprehensive catalog of promoters and enhancers, and all have support from transcriptional evidence. Besides the catalog database, the portal also allows users to upload their genomic loci of interest and analyze the loci with state-of-the-art annotations. The portal is a containerized web service powered by Django, celery, and other open-source packages.","text":"If you are interested in the detailed methodology, please refer to our manuscript, A comparison of experimental assays and analytical methods for genome-wide identification of active enhancers, for more information. Summary Peak Identifier for Nascent Transcript Starts (PINTS) is a software that helps locate potential regulatory elements (promoters and enhancers) genome-wide from nascent transcript sequencing data, like GRO&#x2F;PRO-cap libraries, which are specifically enriched for signals from transcription start sites (TSSs). PINTS web portal is a comprehensive catalog of promoters and enhancers, and all have support from transcriptional evidence. Besides the catalog database, the portal also allows users to upload their genomic loci of interest and analyze the loci with state-of-the-art annotations. The portal is a containerized web service powered by Django, celery, and other open-source packages. AvailabilityPINTS executable files are packed and distributed on both PyPI and bioconda. You can install PINTS like any other standard Python package with pip (pip install pyPINTS) or conda (conda install -c bioconda pypints). The source code is available on GitHub. PINTS web portal is freely accessible at pints.yulab.org. ChangelogPINTS1.1.15Release date: 2024-12-22 [Fixed]: PINTS (boundary extender) may quit when warnings are threw in third-party libraries. 1.1.14Release date: 2024-10-10 [Fixed]: some csi index files not deleted after the entire peak calling process (related to ver1.1.12). 1.1.13Release date: 2024-09-09 [Fixed]: PINTS may quit in cases where the sets of chromosomes in the forward and reverse bigwigs are not 100% identical 1.1.12Release date: 2024-07-21 [Changed]: improved IO performance by up to ~40% when working with bigWig inputs [Changed]: years in comments [Added]: pints_counter for generating count matrix [Fixing]: switched to csi index for better handling of large chromosomes. But more tests regarding memory pressure are needed [Fixed]: csi index files not deleted after the entire peak calling process 1.1.11Skipped 1.1.10Release date: 2023-11-07 [Changed]: Removed warning filters in extension_engine as warnings from other packages may break downstream analysis. [Changed]: years in comments 1.1.9Release date: 2023-05-11 [Added]: A new QC script for evaluating TSS enrichment (over gene body regions) [Changed]: Report scale factors when using pints_visualizer [Changed]: Switched formatter_class for all executable scripts’ argument parsers from HelpFormatter to ArgumentDefaultsHelpFormatter, so that default values will be printed when running commands like pints_caller --help [Fixed]: Added boundary caps to avoid ValueError when calling pysam.TabixFile.fetch (Corresponding to GitHub issue #6) 1.1.8Release date: 2022-10-08 [Fixed]: When no divergent&#x2F;bidirectional peaks are detected, PINTS may throw exceptions. In this version, empty predictions will be skipped, so errors like ERROR: Requested column 12, but database file stdin only has fields 1 - 0. will be avoided (related GitHub issue #12). [Changed]: Since PINTS will automatically adjust the value of --min-mu-percent, now it prints info instead of a warning message when --min-mu-percent related issues are detected. [Changed]: Reduce redundancy in the function peak_calling by abstracting statements about parsing input files. [Added]: Added a new QC metric on the number of significant calls. If the number of significant calls is too large, PINTS will throw a warning message and suggest users switch to a more strict FDR cutoff. 1.1.7Release date: 2022-09-16 [Added]: unit test cases [Added]: Github issue templates [Changed]: Improved the logic of on-the-fly QC. If QC is enabled, now PINTS tries to directly adjust corresponding parameter (for now only --min-mu-percent) and log the changes. This behavior can be repressed by using --disable-qc. 1.1.6Release date: 2022-07-05 [Added]: Added chromosome length check when using BAM files as inputs [Changed]: Simplified on-the-fly QC messages. So warning messages of the same type only show up once at the end of the log file. [Changed]: Dropped peak length limits for empirical LER. [Fixed]: Fixed inappropriate usage of str.find in annotate_tre_with_epig_info. 1.1.5Release date: 2022-06-16 [Fixed]: Add explicit type conversion to avoid AttributeError: Can only use str accessor with string values 1.1.4Release date: 2022-06-13 [Fixed]: fixed typos in readme; [Removed]: Removed scaffold codes; [Changed]: changed default output from pints_boundary_extender to standard bed format [Changed]: moved codes from scripts to PINTS lib for more accurate test coverage analysis; 1.1.0 (1.1.3)Release date: 2022-03-16 (for 1.1.0) and 2022-03-21 (for 1.1.3) [Added]: Source distribution to CI&#x2F;CD for creating conda packages [Added]: Added MANIFEST.in file to include versioneer.py in the source distribution [Added]: empirical LER [Added]: On the fly QC [Changed]: Switched versioning schema from PEP 440 to major_ver.middle_ver.minor_ver. The first two vers will be defined by tags; the minor_ver will be inferred automatically from commit distance. [Changed]: Changed the default value for --alpha (for combining nearby peaks) to 0.3. [Changed]: Renamed README.md. [Changed]: Improved the implementation of independent filtering [Changed]: Improved code readability with Pylint [Removed]: Scaffolding parameters and simplified naming schema. 0.0.1.post0.dev8Release date: 2022-01-07 [Changed]: Add requests to install_requires. 0.0.1.post0.dev7Release date: 2022-01-07 [Changed]: Updated readme and help message 0.0.1.post0.dev6Release date: 2022-01-05 [Fixed]: Fixed timestamp issue when creating log file 0.0.1.post0.dev5Release date: 2022-01-04 [Added]: Introduced version check, which prints update hints if current version of PINTS is out-dated. 0.0.1.post0.dev3Release date: 2021-10-30 [Changed]: Added pyBigWig, biopython, and matplotlib into setup.install_requires. [Added]: Introduced a new module for refining peak calls with epigenomic evidence from PINTS webserver. This feature can be activated by using --epig-annotation &lt;biosample_name&gt;. [Added]: error check for lowly sequenced libraries. 0.0.1.post0.dev2Release date: 2021-08-04 [Added]: Configured CI&#x2F;CD for uploading PINTS to PyPI. [Added]: Added CI&#x2F;CD badge in README. 0.0.1.post0.dev1Release date: 2021-06-03 Initial implementation. PINTS web portal2023v1 [Added]: Included PRO-cap libraries for HCT116 [Changed]: Updated epigenomic annotation to cCRE v4 [Changed]: Updated functional characterization info from NCBI RefSeq [Changed]: Updated TF binding site annotation to JASPAR 2024 2022v1Release date: 2022-07-17 [Added]: Increased biosample coverage by adding PRO-cap libraries for: GM18507, GM19238 from Kristjánsdóttir et al. 2020 (DOI: 10.1038&#x2F;s41467-020-19829-z) MCF7 from Hou et al. 2022 (DOI: 10.1016&#x2F;j.celrep.2022.110944) A673, Caco-2, and MCF 10A from the ENCODE portal ( using DOI: 10.1038&#x2F;s41587-022-01211-7 as a placeholder for now, will update the study link after the manuscript is published) [Changed]: Updated epigenomic annotation to cCRE v3 [Changed]: Updated TF binding site annotation to JASPAR 2022 [Changed]: Updated functional characterization info for HepG2 by including consistent enhancer calls from Klein et al. 2020. [Changed]: Switched to PINTS 1.1.6. [Deprecated]: Removed bidirectional elements identified from RAMPAGE&#x2F;CAGE libraries that overlap with protein-coding exons (except $5^\\prime$ UTR). 2021v1Release date: 2021-10-23 Initial release.","categories":[{"name":"tools","slug":"tools","permalink":"https://www.yaobio.com/tools/"}],"tags":[]},{"title":"BTRY 4381/6381 Biomedical Data Mining and Modeling","slug":"teaching/2021-07-04-teaching-BTRY6381","date":"2020-09-01T00:00:00.000Z","updated":"2025-04-23T04:03:22.948Z","comments":true,"path":"/teaching/btry-4381-6381-biomedical-data-mining-and-modeling/","link":"","permalink":"https://www.yaobio.com/teaching/btry-4381-6381-biomedical-data-mining-and-modeling/","excerpt":"","text":"A biomedical data science course using Python and available bioinformatics tools and techniques for the analysis of molecular biological data, including biosequences, microarrays, and networks. This course emphasizes practical skills rather than theory. Topics include advanced Python programming, R and Bioconductor, sequence alignment, MySQL database (DBI), web programming and services (CGI and django), genomics and proteomics data mining and analysis, machine learning, and methods for inferring and analyzing regulatory, protein-protein interaction, and metabolite networks.","categories":[{"name":"teaching","slug":"teaching","permalink":"https://www.yaobio.com/teaching/"}],"tags":[]},{"title":"Mastering IGV: Tips and Tricks for Visualizing Sequencing Data","slug":"posts/2020-08-01-IGV","date":"2020-08-01T19:08:23.000Z","updated":"2025-04-23T04:03:22.944Z","comments":true,"path":"/posts/mastering-igv-tips-and-tricks-for-visualizing-sequencing-data/","link":"","permalink":"https://www.yaobio.com/posts/mastering-igv-tips-and-tricks-for-visualizing-sequencing-data/","excerpt":"","text":"If a human being had actually looked at his blood, anywhere along the way, instead of just running tests through the computer… parasites would have jumped right out at them. “Failure to Communicate.” House M.D. There is a case in the TV show House M.D., where a parasite infected the patient; Dr. House’s team runs many tests and gets no clue. In the end, Dr. House tells the team to look at the blood sample through a microscope instead of using numbers from instruments. The team does so and instantly sees the parasite. Even though the settings of the disease may not be very accurate, it leaves me with a profound impression. The same situation can also happen in bioinformatics, as we do many tests to identify targets of interest or evaluate our confidence in conclusions. However, we should be aware that these tests are all based on some assumptions, and it’s always beneficial to visually check the data to have an intuitive impression that the data fit the assumptions. Integrated Genome Browser (IGV) is a powerful tool for visualizing sequencing data. In this post, I’ll share some of my tricks for making IGV even more useful. Case 1: There are a bunch of loci that need to be checked visuallyIf you need to visually check many regions of interest in IGV, you can use a batch script to automate the process. The IGV batch script language allows you to generate a script file that tells IGV which regions to display and where to save the output. Instead of learning the details about this minimal language yourself, you can directly use bedtools to create a batch script for a list of loci in a bed file. In the following example, genomic loci in loci.bed will first be extended 200 bp both upstream and downstream, then a batch script covering these regions will be saved to the file batch.script. 123456789# snapshots will be saved to `path_to_store_snapshots`# -slop indicates the number of flanking base pairs on # both the left and right of the interested regions to be # extended in the captured images (0 for keep them as they are)# I recommend set the output img format to be svg or eps# the resolution for png file is so lowbedtools igv -path path_to_store_snapshots \\ -i loci.bed -slop 200 \\ -img svg &gt; batch.script After loading the tracks you want to see in IGV, click Tools&gt;Run Batch Script... and load the batch.script file, IGV will get start capturing snapshots of each locus. If the process is slow, you can split the bed file and generate batch scripts for each subset, then load them into separate instances of IGV. Case 2: Frequently used annotations are not listed in the default serverThe IGV team maintains a fabulous web server with some commonly used annotations (like gene annotations from the GENCODE project) or datasets (like ChIP-seq alignments from the ENCODE project); by simply selecting the annotations of interest from File&gt;Load from Server..., you can load them to your current session. One small pitfall with this function is that the annotations or datasets are not always up-to-date; for some frequently used files (or customized files), you may want them listed there. In this case, you should consider setting up your data server for IGV. Step 1: Copy precompiled data files from IGVYou can get a copy of all genome files that IGV is currently using from their GitHub repo: 12345git clone https://github.com/igvteam/igv.git# igv team removed genome files in commit 218f873# so we need to check out from one commit before the deletion,# which refers to commit beb4f48git checkout beb4f48e04 After checkout, you can copy the entire igv/genomes folder to a new place (assuming it’s /nas1/references) and set up your data server. Step 2: Install and configure a web serverIf you’ve already had a web server, then you can move to step 3. For Mac users, you can install Nginx with Homebrew: 1brew install nginx By default, the configuration file for Nginx (installed by Homebrew) is located at /usr/local/etc/nginx/nginx.conf. In the http section, add a new server configuration as follows: 1234567891011121314server &#123; listen 80; listen [::]:80; # if you have a domain, replace `ref.yaobio.com` with your own domain # if you don&#x27;t have one, and only want to access the web server locally # you can replace it with localhost server_name ref.yaobio.com; location / &#123; # replace this path with where you put the data files from IGV root /nas1/references; index index.html index.htm; &#125;&#125; Reload the configurations to make changes effective: 1nginx reload Create a new directory (annotations) in /nas1/references. Now the structure of this folder is something like references db 1kg_ref … hg19 hg38 mm10 … sizes 1kg_ref.chrom.sizes … hg19.chrom.sizes hg38.chrom.sizes mm10.chrom.sizes … annotations genomes.tab genomes.txt Step 3: Save new annotations and modify data filesLet’s assume you have a new annotation file processed (e.g., processed GENCODE v35 for hg38 with the pipeline we mentioned in the previous post); now, you can move the file to /nas1/references/annotations. Then you need to modify the default genome and data registry: Change the content of db/hg38/hg38_dataServerRegistry.txt from https://s3.amazonaws.com/igv.org.genomes/hg38/hg38_annotations.xml to https://ref.yaobio.com/db/hg38/hg38_annotations.xml Add the following item to db/hg38/hg38_annotations.xml: 1&lt;Resource name=&quot;Gencode V35&quot; path=&quot;http://ref.yaobio.com/annotations/gencode.v35.annotation.sorted.gtf.gz&quot; index=&quot;http://ref.yaobio.com/annotations/gencode.v35.annotation.sorted.gtf.gz.tbi&quot; hyperlink=&quot;http://www.gencodegenes.org/&quot;/&gt; Step 4: Change the data server setting in IGVNow in IGV, click View&gt;Preferences&gt;Advanced, and replace the previous value in Data registry url with http://ref.yaobio.com/db/$$/$$_dataServerRegistry.txt. Finally, save the changes, and restart IGV; you should be able to see and load newly added annotations into IGV. General tipsAlways load bed files with indicesAlways create indices for bed files before loading them into IGV. Otherwise, IGV will read every interval into memory and generate indexes on the fly, consuming excessive memory and time. You can use tabix to generate an index for interval files before loading them into IGV. This practice can greatly reduce memory usage and computation time. Let’s say we have an interval bed file test_file_1.bed.gz, it has 18M records; after loading this file into IGV without index, IGV takes more than 25GB of memory! But if you use tabix test_file_1.bed.gz to generate the index first, and then feed IGV with the same file, it only takes 2GB! ConclusionIGV is a valuable tool for visualizing sequencing data, and these tips and tricks can help you make the most of its capabilities. By using batch scripting, setting up a personal data server, and optimizing bed file loading with indices, you can streamline your bioinformatics workflows and gain more insights from your data.","categories":[{"name":"posts","slug":"posts","permalink":"https://www.yaobio.com/posts/"}],"tags":[{"name":"technical_docs","slug":"technical-docs","permalink":"https://www.yaobio.com/tags/technical-docs/"}]},{"title":"Setting up a handy research environment on Linux servers","slug":"posts/2020-07-20-Setup-Linux-Server","date":"2020-07-20T09:31:47.000Z","updated":"2025-04-23T04:03:22.942Z","comments":true,"path":"/posts/setting-up-a-handy-research-environment-on-linux-servers/","link":"","permalink":"https://www.yaobio.com/posts/setting-up-a-handy-research-environment-on-linux-servers/","excerpt":"","text":"Login with SSH keyPutting your password every time you log into a server via SSH can be tiresome, especially if your password is long. One way to make life easier and safer is by using SSH keys. In the terminal of your local machine, type in ssh-keygen. By default, the keys will be exported to ~/.ssh/id_rsa; using the default is okay in most cases, but if you have multiple keys and want to avoid conflicts, you can change the destination. When it prompts you to enter a passphrase, you can click Enter (empty passphrase) so that you don’t need to input anything when you log in to a server using this key pair. But for better security, a passphrase is suggested. Copy your SSH public key into the server by ssh-copy-id -i ~/.ssh/id_rsa.pub user_name@server_address (change the path to the public key if you’ve saved it somewhere else). You’ll be prompted to enter your password at the server. Log in to the server by ssh user_name@server_address. You should be able to log in without having to enter your password! If it doesn’t work, check the permission of the ~/.ssh folder on your server. Only you should have write access to it. You can change its permission from your home folder by chmod 700 .ssh. Avoid file descriptor exhaustionRunning pipelines requires handle large numbers of concurrent connections require the ability to open many files (sockets, log files, etc.). The default ulimit values (especially for file descriptors) are often too low for research servers. Without adjusting these limits, File descriptor exhaustion can occur, resulting in errors like “Too many open files.” Check the max allowed number of open files by 1ulimit -n Set it to a larger number: 1ulimit -n 100000 Or add the following line to /etc/security/limits.conf: 1* - nofile 100000 Change the default plotting settings for Matplotlib and JupyterMany people explore and analyze data with the powerful combination of Matplotlib (along with Seaborn) and Jupyter Notebook&#x2F;Lab. This widely favored choice provides a flexible environment for data visualization. However, it’s important to note that the default settings of these tools may not always meet field-specific requirements. To address this, I have compiled a list of modifications that I personally introduced to the configuration files of Jupyter and Matplotlib. These changes allow for customization and tailoring of their default behaviors to suit individual needs better. Change default font styles in MatplotlibScientific publishers often have customized guidelines for figure fonts to ensure optimal print readability. They may specify requirements such as setting the typeface as Arial or Helvetica, with a minimum font size of 5 pt and a maximum size of 7 pt. Unfortunately, the default font styling in Matplotlib&#x2F;Seaborn is optimized for screen reading. As a result, many researchers manually adjust the font sizes of their generated figures in image editing software like Illustrator or Affinity Designer. This tedious process can be time-consuming and prone to error. By making necessary modifications to the configuration files of Jupyter and Matplotlib, we can ensure that our plots are in a publication-ready state from the very beginning, saving valuable time and effort. Here, I will show you how to change the default font size for matplotlib: Locate the configuration file for matplotlib by running the following code in a Python session:12import matplotlibprint(matplotlib.matplotlib_fname()) Open the configuration file, locate the default settings that you want to override. figure.titlesize: Font size of the figure title axes.labelsize: Font size for $x$ and $y$ labels xtick.labelsize and ytick.labelsize: Font size for the $x$ or $y$ ticks legend.title_fontsize: Font size for the title of the legend legend.fontsize: Font size for legends If there’s a # right before the option, it means this option is commented and will not be considered by matplotlib. So to make it effective, remove the # first, then change the values to the ones you desired. Save the file, and go create new figures. Here is the actual settings that I usually use: 12345678910font.sans-serif: Arial, Helvetica, DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Avant Garde, sans-seriffont.size: 5 # default text sizesfigure.titlesize: 7 # size of the figure title (``Figure.suptitle()``)figure.labelsize: 7 # size of the figure label (``Figure.sup[x|y]label()``)axes.titlesize: 7 # font size of the axes titleaxes.labelsize: 7 # font size of the x and y labelsxtick.labelsize: 6 # font size of the x tick labelsytick.labelsize: 6 # font size of the y tick labelslegend.title_fontsize: 7 # font size of legend tilelegend.fontsize: 6 # font size of other text in the legend If you are looking for a one-time change, you can override these values in RcParams: 1234567plt.rc(&#x27;font&#x27;, size=5) # controls default text sizesplt.rc(&#x27;axes&#x27;, titlesize=7) # fontsize of the axes titleplt.rc(&#x27;axes&#x27;, labelsize=7) # fontsize of the x and y labelsplt.rc(&#x27;xtick&#x27;, labelsize=6) # fontsize of the tick labelsplt.rc(&#x27;ytick&#x27;, labelsize=6) # fontsize of the tick labelsplt.rc(&#x27;legend&#x27;, fontsize=6) # legend fontsizeplt.rc(&#x27;figure&#x27;, titlesize=7) # fontsize of the figure title Note: if you set the font sizes to 5~7 pts, you may also need to scale down the figure size. The default figure size is $6.4\\times4.8 $ inches, I usually set the default as: 1figure.figsize: 3.2, 2.4 # figure size in inches Make figures in Notebook more clearFor high-definition screen users, especially for people who are using Macbook and iMac, the inline figures shown up in Jupyter Notebook&#x2F; Lab can be very blurry, like the this one: To solve this problem, you can execute the following command in the notebook that you want to get high-resolution figures: 1%config InlineBackend.figure_format=&#x27;retina&#x27; Now output figures will be much more clear: The major drawback for the above method is that you have to execute it every time you create a new Notebook. If you want Notebooks to produce high-def figures by default, you’ll need to modify the configuration files for IPython (which Notebooks use it to run actual codes): Check if you’ve already had a configuration (ipython_kernel_config.py) for IPython created before. For Linux and MacOS users, the file is usually located at ~/.ipython/profile_default, if you don’t know where it locates, you can use ipython locate to figure it out; if the file is not in this destination, you can run ipython profile create to create it. Add c.InlineBackend.figure_formats = [&quot;retina&quot;] or c.InlineBackend.figure_formats = [&quot;svg&quot;] to the end of this file (ipython_kernel_config.py). Kill your Jupyter Notebook &#x2F; Lab, and rerun it.","categories":[{"name":"posts","slug":"posts","permalink":"https://www.yaobio.com/posts/"}],"tags":[{"name":"technical_docs","slug":"technical-docs","permalink":"https://www.yaobio.com/tags/technical-docs/"},{"name":"orientation","slug":"orientation","permalink":"https://www.yaobio.com/tags/orientation/"}]},{"title":"Securing Jupyter Notebooks by tracing modification histories","slug":"posts/2020-05-15-jupyter-history","date":"2020-05-15T19:45:21.000Z","updated":"2025-04-23T04:03:22.941Z","comments":true,"path":"/posts/securing-jupyter-notebooks-by-tracing-modification-histories/","link":"","permalink":"https://www.yaobio.com/posts/securing-jupyter-notebooks-by-tracing-modification-histories/","excerpt":"","text":"Jupyter notebook&#x2F;lab is a fantastic tool for data analysis. I used it a lot in my daily research. But one of my habits makes me suffered a lot: I keep updating the same notebook for a long time, and to make the notebook neat, I always delete some of the cells, which are “redundant”&#x2F;“useless” for me at certain time points (or I just delete them by accident). So in many cases, I wondered whether I could add version control to notebooks, so that when something wrong happens, I can rollback to the status before specific events. Start to secure your notebooksThe basic idea is to add a hook to Jupyter whenever it finishes saving a file (especially notebook), which will: generate a git repo in the folder where the notebook locates (if there isn’t); copy the notebook to the repo (and convert notebook to python script); add all changes to git; commit modifications. Note: In many cases, we will focus in a short time and make a lot of modifications to notebooks. To avoid making too many commits, it would be better if we can randomly skip some modifications. In this post, let’s denote time span (in seconds) between last commit and current modification as $ts$, then the probability is $\\min(1, \\frac{ts}{3600})$ for the modified notebook to get committed. Here are the steps to enable automatical backup (MUST install git first): Make sure you have a configuration file (jupyter_notebook_config.py) for Jupyter Notebook or Jupyter Lab. By default, it’s located at ~/.jupyter. If are not sure, you can run jupyter --config-dir, which will tell you the file locates. If you don’t have any configuration files, generate one with jupyter notebook --generate-config; Add the following codes to the beginning of the configuration file:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import osfrom time import ctime, timefrom random import choicesfrom subprocess import check_call, CalledProcessErrorfrom shutil import copyfiledef post_save(model, os_path, contents_manager): &quot;&quot;&quot; This function will do the following jobs: 1. creating a new folder named vcs (version control system) 2. converting jupyter notebooks to both python scripts and html files 3. moving converted files to vcs folder 4. keeping tracks with these files via git &quot;&quot;&quot; if model[&#x27;type&#x27;] != &#x27;notebook&#x27;: return logger = contents_manager.log d, fname = os.path.split(os_path) # skip new born notebooks if fname.startswith(&quot;Untitled&quot;): return vcs_d = os.path.join(d, &quot;vcs&quot;) if not os.path.exists(vcs_d): logger.info(&quot;Creating vcs folder at %s&quot; % d) os.makedirs(vcs_d) try: check_call([&#x27;git&#x27;, &#x27;rev-parse&#x27;], cwd=vcs_d) except CalledProcessError: logger.info(&quot;Initiating git repo at %s&quot; % d) check_call([&#x27;git&#x27;, &#x27;init&#x27;], cwd=vcs_d) def add_file_or_not(file_name, folder): file_path = os.path.join(folder, file_name) if os.path.exists(file_path): ctime = os.path.getctime(file_path) delta = time() - ctime delta = delta if delta &gt;= 0 else 0 # if file was modified in the past 1 hour, # then the new modification got 30% chance # to be saved prob = delta / 3600 prob = prob if prob &lt;= 1 else 1 save_or_not = choices((0, 1), weights=(1-prob, prob), k=1)[0] if save_or_not: return True else: return False else: return True rfn, ext = os.path.splitext(fname) # in case notebook is not a python-based one (R,...) script_ext = &quot;.py&quot; with open(os_path) as fh: tmp = json.load(fh) script_ext = tmp[&quot;metadata&quot;][&quot;language_info&quot;][&quot;file_extension&quot;] script_fn = rfn+script_ext if add_file_or_not(script_fn, d): check_call([&#x27;jupyter&#x27;, &#x27;nbconvert&#x27;, &#x27;--to&#x27;, &#x27;script&#x27;, fname], cwd=d) os.replace(os.path.join(d, script_fn), os.path.join(d, &quot;vcs&quot;, script_fn)) copyfile(os_path, os.path.join(d, &quot;vcs&quot;, fname)) check_call([&#x27;git&#x27;, &#x27;add&#x27;, script_fn, fname], cwd=vcs_d) commit_msg = &#x27;Autobackup for %s (%s)&#x27; % (fname, ctime()) try: check_call([&#x27;git&#x27;, &#x27;commit&#x27;, &#x27;-m&#x27;, commit_msg], cwd=vcs_d) except CalledProcessError: pass else: logger.info(&quot;File too new to be traced.&quot;) Search for post_save_hook in the configuration file; uncomment this line (if you see a # at the begining of the line, remove it), and change it to:1c.FileContentsManager.post_save_hook = post_save Restart Jupyter Lab or Notebook. Restore from disastersThe easiest way to restore jupyter notebook from backups is add the vcs folder to a GUI for git (like Sourcetree) and export the version that you want to roll back to. But in case GUIs are not available, you can use the following commands. Let’s assume the notebook is a.ipynb, and it locates at /foo/bar, then the git repo is located at /foo/bar/vcs, you can: use git log --abbrev-commit to see the history of modifications; choose the potential commit according to the time of submission, and record the commit id (red boxes above); use git show COMMIT_ID:FILE_NAME &gt; SAVE_TO to export the committed file to SAVE_TO. If you want to export the records jupyter, then in this case, you can replace FILE_NAME as a.ipynb; but if you just want to export python codes, then you can replace FILE_NAME with a.py. A more comprehensive implementationA better implementation of this function is to enable: backup&#x2F;version control only in specific folders instead of everywhere; tracing other non-notebook files, like Python&#x2F;R scripts; allowing pushing changes to remote repositories. So by introducing the following two changes, the above three goals can be meet smoothly: Add a configuration file (backup.conf) in the folder that you want to enable version control, and below is a template for this file123456789[repo]local_repo_folder = vcsremote_repo = [backup]backup_notebooks = nobackup_notebook_converted_scripts = yesbackup_by_filetypes = yesbackup_filetypes = .py|.R The keys in the configuration file stand for: local_repo_folder: name of the local folder that the git repo will be located; remote_repo: remote address for the repo, leave it as blank if you don’t want to push these changes to other servers; backup_notebooks: set it to yes to enable tracing Notebooks in .ipynb format. NOTE: if your notebook contains a lot images or significant amount of outputs, then you may want to set it as no to save some space; backup_notebook_converted_scripts: set it to yes to enable tracing Notebooks in .py format or any other format that the notebook is based-on; backup_by_filetypes and backup_filetypes: set the first one to be yes to enable tracing other text files with extensions in backup_filetypes. In this example, all files ending with .py and .R will be traced (case-insensitive). Modify post_save:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import osimport jsonfrom configparser import ConfigParserfrom time import ctime, timefrom random import choicesfrom subprocess import check_call, CalledProcessErrorfrom shutil import copyfiledef post_save(model, os_path, contents_manager): &quot;&quot;&quot; This function will do the following jobs: 1. creating a new folder named vcs (version control system) 2. converting jupyter notebooks to both python scripts and html files 3. moving converted files to vcs folder 4. keeping tracks with these files via git &quot;&quot;&quot; d, fname = os.path.split(os_path) conf_file = os.path.join(d, &quot;backup.conf&quot;) if not os.path.exists(conf_file): return config = ConfigParser() config.optionxform = str config.read(conf_file) supported_files = set([ft.lower() for ft in config.get(&quot;backup&quot;, &quot;backup_filetypes&quot;).split(&quot;|&quot;)]) logger = contents_manager.log vcs_d = os.path.join(d, config.get(&quot;repo&quot;, &quot;local_repo_folder&quot;)) if not os.path.exists(vcs_d): logger.info(&quot;Creating vcs folder at %s&quot; % d) os.makedirs(vcs_d) repo_init = 0 try: check_call([&#x27;git&#x27;, &#x27;rev-parse&#x27;], cwd=vcs_d) except CalledProcessError: logger.info(&quot;Initiating git repo at %s&quot; % d) check_call([&#x27;git&#x27;, &#x27;init&#x27;], cwd=vcs_d) if config.get(&quot;repo&quot;, &quot;remote_repo&quot;) != &quot;&quot;: check_call([&#x27;git&#x27;, &#x27;remote&#x27;, &#x27;add&#x27;, &#x27;origin&#x27;, config.get(&quot;repo&quot;, &quot;remote_repo&quot;)], cwd=vcs_d) repo_init = 1 def add_file_or_not(file_name, folder): file_path = os.path.join(folder, file_name) if os.path.exists(file_path): ctime = os.path.getctime(file_path) delta = time() - ctime delta = delta if delta &gt;= 0 else 0 # if file was modified in the past 1 hour, # then the new modification got 30% chance # to be saved prob = delta / 3600 prob = prob if prob &lt;= 1 else 1 save_or_not = choices((0, 1), weights=(1-prob, prob), k=1)[0] if save_or_not: return True else: return False else: return True rfn, ext = os.path.splitext(fname) lext = ext.lower() updated_files = [] if model[&quot;type&quot;] == &quot;notebook&quot;: # skip new born notebooks if fname.startswith(&quot;Untitled&quot;): return # in case notebook is not a python-based one (R,...) script_ext = &quot;.py&quot; with open(os_path) as fh: tmp = json.load(fh) script_ext = tmp[&quot;metadata&quot;][&quot;language_info&quot;][&quot;file_extension&quot;] script_fn = rfn + script_ext if add_file_or_not(script_fn, d): if config.get(&quot;backup&quot;, &quot;backup_notebooks&quot;) == &quot;yes&quot;: copyfile(os_path, os.path.join(vcs_d, fname)) updated_files.append(fname) if config.get(&quot;backup&quot;, &quot;backup_notebook_converted_scripts&quot;) == &quot;yes&quot;: check_call([&#x27;jupyter&#x27;, &#x27;nbconvert&#x27;, &#x27;--to&#x27;, &#x27;script&#x27;, fname], cwd=d) os.replace(os.path.join(d, script_fn), os.path.join(vcs_d, script_fn)) updated_files.append(script_fn) else: logger.info(&quot;File too new to be traced.&quot;) elif config.get(&quot;backup&quot;, &quot;backup_by_filetypes&quot;) == &quot;yes&quot; and lext in supported_files: copyfile(os_path, os.path.join(vcs_d, fname)) updated_files.append(fname) if len(updated_files) &gt; 0: cmd = [&#x27;git&#x27;, &#x27;add&#x27;] cmd.extend(updated_files) check_call(cmd, cwd=vcs_d) commit_msg = &#x27;Autobackup for %s (%s)&#x27; % (fname, ctime()) try: check_call([&#x27;git&#x27;, &#x27;commit&#x27;, &#x27;-m&#x27;, commit_msg], cwd=vcs_d) if config.get(&quot;repo&quot;, &quot;remote_repo&quot;) != &quot;&quot;: if repo_init: check_call([&#x27;git&#x27;, &#x27;push&#x27;, &#x27;--set-upstream&#x27;, &#x27;origin&#x27;, &#x27;master&#x27;], cwd=vcs_d) else: check_call([&#x27;git&#x27;, &#x27;push&#x27;], cwd=vcs_d) except CalledProcessError as e: logger.info(e) References A closed issue from Jupyter Hub nbconvert","categories":[{"name":"posts","slug":"posts","permalink":"https://www.yaobio.com/posts/"}],"tags":[{"name":"technical_docs","slug":"technical-docs","permalink":"https://www.yaobio.com/tags/technical-docs/"}]},{"title":"Demystifying NGS Quality Reports: Case Studies","slug":"posts/2020-05-10-ngs-qc","date":"2020-05-10T09:11:34.000Z","updated":"2025-04-23T04:03:22.936Z","comments":true,"path":"/posts/demystifying-ngs-quality-reports-case-studies/","link":"","permalink":"https://www.yaobio.com/posts/demystifying-ngs-quality-reports-case-studies/","excerpt":"Unlocking the secrets hidden in Next-Generation Sequencing (NGS) data is an exciting journey, but it’s important to ensure that the data quality is top-notch. That’s where powerful tools like FastQC come in! With FastQC, you can easily perform quality checks and ensure that your data is of the highest caliber. In this post, we delve into real-life cases where diagnostic plots from FastQC have helped unravel complex problems and bring clarity to our analysis. Join me on this thrilling adventure!","text":"Unlocking the secrets hidden in Next-Generation Sequencing (NGS) data is an exciting journey, but it’s important to ensure that the data quality is top-notch. That’s where powerful tools like FastQC come in! With FastQC, you can easily perform quality checks and ensure that your data is of the highest caliber. In this post, we delve into real-life cases where diagnostic plots from FastQC have helped unravel complex problems and bring clarity to our analysis. Join me on this thrilling adventure! The arrangement of this post is based on the output from FastQC, but the criteria or cases mentioned here should also be applicable to similar measurements produced by other tools. Per Base Sequence ContentPer Base Sequence Content stacks together all sequences in a fastq file and calculates the frequency of the four normal DNA bases (A&#x2F;T&#x2F;C&#x2F;G) called for each base position. In a random or unbiased library, the bases should follow a random distribution of A&#x2F;T&#x2F;C&#x2F;G, so the lines in the plot should be parallel to each other (around 25%). However, the actual distributions may have some fluctuations, depending on the overall number of bases in the genome and the capturing bias from the assay. Nevertheless, in most cases, the lines should be approximately parallel. Per Base Sequence Content can be used to find: biased fragments, like: untrimmed barcodes. For demultiplexed libraries, if there are untrimmed barcodes, then because of the fixed sequences, you would observe sharp peaks at the beginnings or ends of reads. Below is an example showing that $5^\\prime$ barcodes (TGGTCAC) are not trimmed: template switching oligo. In cases like this, you can observe characteristic trinucleotide GGG or CCC near the beginning of reads. overrepresented sequences, like adapter dimers or rRNAs. Safelist: For libraries treated with sodium bisulfite, which will convert C to T, it’s normal to observe a low percent of Cs. Adapter ContentFor libraries where a significant amount of the inserts are shorter than the sequencing length, adapters are likely to be incorporated in final reads. This is very common for libraries enriching for short&#x2F;small RNAs, like PRO-cap, PRO-seq, etc. The Adapter Content module compares reads with commonly used adapter sequences and plots the enrichment. Adapter sequences may greatly affect on sequencing alignments, so if you see warnings in this section, you may need to trim adapters with cutadapt, fastp, or any other tool you like. In the following example, the Adapter Content module detects the existence of Nextera Transposase Sequence (the risen black line). Overrepresented SequencesA sequencing library typically consists of a diverse mixture of DNA or RNA molecules, and the presence of frequently occurring specific sequences can indicate an abnormality. Possibility 1: AdaptersOne possible cause for warnings or errors from this module is the presence of adapters. For example, people usually use a customized $3^\\prime$ cloning adapter, CTGTAGGCACCATCAAT, to generate Ribo-seq libraries. This sequence is not included in the known-adapter list in FastQC, so the Adapter Content module cannot detect the existence of adapters, but the Overrepresented Sequences module catches a lot of hits. The following screenshot shows the top 12 overrepresented sequences from a Ribo-seq library (SRR942878), and the adapter sequence is highlighted for clarity. In practical applications, adapters may not be immediately visible in a table of many sequences. To identify potential adapters, I often choose a few overrepresented sequences and use the Smith-Waterman alignment algorithm to find adapter candidates. Typically, adapters appear as aligned contigs near the ends of sequences. For instance, when running the SW alignment on the first two hits in the table above, the adapter CTGTAGGCACCATCAAT is clearly visible: 12CCGGCTAGCTCAGTCGGTAGAGCATGAGCTGTAGGCACCATCAATTCGCCAGCTAGCA-ATTGGGTGTAGC-----CTGTAGGCACCATCAATTCG You can run the Smith-Waterman algorithm here. Possibility 2: rRNAs or Other “Contamination”What if the overrepresented sequences don’t match the typical adapter patterns? In some cases, these sequences may actually be “contamination” from ribosomal RNAs or other sources. One way to test this possibility is to run a BLAST search on the sequences. For example, here are the top overrepresented sequences for another sequencing library: When running BLAST on some of these overrepresented sequences, you can see that they match with ribosomal RNAs. 123456789Chain 2, 18S rRNASequence ID: 7WTT_2 Length: 1873 Number of Matches: 1Range 1: 131 to 182Alignment statistics for match #1Score Expect Identities Gaps Strand97.1 bits(52) 1e-16 52/52(100%) 0/52(0%) Plus/MinusQuery 1 GTCGGCATGTATTAGCTCTAGAATTACCACAGTTATCCAAGTAGGAGAGGAG 52 ||||||||||||||||||||||||||||||||||||||||||||||||||||Sbjct 182 GTCGGCATGTATTAGCTCTAGAATTACCACAGTTATCCAAGTAGGAGAGGAG 131 If this is the case, you can align the entire library to ribosomal RNAs first, then do a second round of alignment with reads that cannot be aligned in the first round. This ensures you are left with high-quality reads for downstream analysis. Per Base Sequence QualityNo cases yet. Per Sequence Quality ScoresNo cases yet. Per Sequence GC ContentNo cases yet. Per Base N ContentNo cases yet. Sequence Length DistributionNo cases yet. Sequence Duplication LevelsNo cases yet. Reference: FastQC manual","categories":[{"name":"posts","slug":"posts","permalink":"https://www.yaobio.com/posts/"}],"tags":[{"name":"technical_docs","slug":"technical-docs","permalink":"https://www.yaobio.com/tags/technical-docs/"}]},{"title":"File templates for writing codes","slug":"posts/BestPractice/Scripts","date":"2020-04-29T17:17:37.000Z","updated":"2025-04-23T04:03:22.945Z","comments":true,"path":"/posts/file-templates-for-writing-codes/","link":"","permalink":"https://www.yaobio.com/posts/file-templates-for-writing-codes/","excerpt":"","text":"Some good templates for writing codes (shell, python, etc) were provided in this post. Shell script Specify shebang in the beginning of shell script, which tells the loader which interpreter should be called, this helps as different bashes have different syntaxes. In the following case, shebang line tells the loader to use user-preferred bash. Add error handling: set -o errexit;, this will abort the execution of current shell script, if any command fails to be executed. When something wrong happens, this strategy helps you to locate the problem quickly, as traceback information will be highlighted at the line that the error actually occurs. 12#!/usr/bin/env bashset -o errexit; Python script Specify shebang in the beginning of python scripts, if you want to run the script directly ./one_script.py. Define Python source code encodings so that non-Latin characters will not disrupt the execution of a script. 12#!/usr/bin/env python# coding=utf-8","categories":[{"name":"posts","slug":"posts","permalink":"https://www.yaobio.com/posts/"}],"tags":[{"name":"best_practice","slug":"best-practice","permalink":"https://www.yaobio.com/tags/best-practice/"}]},{"title":"Collection of commonly used references in bioinformatic analysis","slug":"posts/references","date":"2020-04-22T17:41:22.000Z","updated":"2025-04-23T04:03:22.945Z","comments":true,"path":"/posts/collection-of-commonly-used-references-in-bioinformatic-analysis/","link":"","permalink":"https://www.yaobio.com/posts/collection-of-commonly-used-references-in-bioinformatic-analysis/","excerpt":"","text":"HumanCurrently, there are two widely used releases GRCh38 (hg38) and GRCh37 (hg19). GRCh38 (hg38)Sequences File name: GRCh38_no_alt_analysis_set_GCA_000001405.15.fa.gz (MD5 checksum: a08035b6a6e31780e96a34008ff21bd6) Local path: &#x2F;References&#x2F;Sequences&#x2F;human&#x2F;hg38&#x2F;GRCh38_no_alt_analysis_set_GCA_000001405.15.fa.gz Remote backup: OSF Description: This file contains sequences for the following: chromosomes from the GRCh38 Primary Assembly (PA); mitochondrial genome from the GRCh38 non-nuclear assembly; unlocalized scaffolds from PA; unplaced scaffolds from PA; Epstein-Barr virus (EBV) sequence. Recipe:1wget https://www.encodeproject.org/files/GRCh38_no_alt_analysis_set_GCA_000001405.15/@@download/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta.gz; AnnotationsGene annotationsThere are three major releases of gene annotations for Homo sapiens: GENCODE&#x2F;Ensembl annotation: The GENCODE annotation is made from Ensembl annotation, so gene annotations are the same in both releases. The only exception is that the genes which are common to the human chromosome X and Y PAR regions can be found twice in the GENCODE GTF, while they are shown only for chromosome X in the Ensembl file. Gene &#x2F; transcripts IDs are the same in both releases except for annotations is the PAR regions. Comparing to other annotations, GENCODE annotation provides higher coverage among non-coding regions. RefSeq Gene (RefGene): Annotations for well-characterized genes (mostly protein-coding genes). Projects like Gene Ontology, KEGG and MSigDB (Molecular Signatures Database, gene sets for GSEA) use this annotation as gene identifiers. So RefGene maybe the preferred annotation if you want to do enrichment analysis with GO&#x2F;KEGG&#x2F;GSEA. UCSC Known genes: Automatically generated annotations (based on protein sequences from Swiss-Prot), mostly for protein-coding genes. GENCODE File name: gencode.v24.annotation.gtf.gz (MD5 checksum: 17395005bb4471605db62042b992893e) Local path: &#x2F;References&#x2F;Annotations&#x2F;human&#x2F;hg38&#x2F;gencode.v24.annotation.gtf.gz Remote backup: OSF Description: GENCODE comprehensive annotation release 24. Downloaded from GENCODE’s website. Recipe: 1wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_24/gencode.v24.annotation.gtf.gz File name: gencode.v24.segmented.tssup1kb.bed.gz (MD5 checksum: 972a57431c6209667d5aac41bbb01ebd) Local path: &#x2F;References&#x2F;Annotations&#x2F;human&#x2F;hg38&#x2F;gencode.v24.segmented.tss*up1kb.bed.gz* Remote backup: OSF Description: Genomic segmentations (promoter, 5_UTR, exon, intron, 3_UTR, and intergenic region) based on GENCODE v24, promoters were defined as upstream 1kb of TSSs (transcripts). Recipe: 12345678910# promoters for protein-coding geneszcat gencode.v24.annotation.gtf.gz | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;&#125; $3==&quot;transcript&quot; &#123;print $1,$4-1,$5,$18,&quot;promoter&quot;,$7,$14&#125;&#x27; | tr -d &#x27;&quot;;&#x27; | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;FS=&quot;\\t&quot;&#125;&#123;if ($7==&quot;protein_coding&quot;)&#123;print $1,$2,$3,$4,$5,$6,$2,$3,&quot;102,194,165&quot;&#125;&#125;&#x27; | \\ bedtools flank -i - -g hg38.genome -l 1000 -r 0 -s &gt; promoters_1kb_p.bed# promoters for non-protein-coding geneszcat gencode.v24.annotation.gtf.gz | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;&#125; $3==&quot;transcript&quot; &#123;print $1,$4-1,$5,$18,&quot;promoter(NP)&quot;,$7,$14&#125;&#x27; | tr -d &#x27;&quot;;&#x27; | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;FS=&quot;\\t&quot;&#125;&#123;if ($7!=&quot;protein_coding&quot;)&#123;print $1,$2,$3,$4,$5,$6,$2,$3,&quot;102,194,165&quot;&#125;&#125;&#x27; | \\ bedtools flank -i - -g hg38.genome -l 1000 -r 0 -s &gt; promoters_1kb_np.bed File name: gencode.v24.segmented.tssflanking500b.bed.gz Local Path: &#x2F;References&#x2F;Anotations&#x2F;human&#x2F;hg38&#x2F;gencode.v24.segmented.tss*flanking500b.bed.gz* Remote backup: OSF Description: Genomic segmentations based on GENCODE v24, promoters were defined as TSS $\\pm$ 500bp (transcripts). (2e624c3bc2330beb81464558ead1a11e) Recipe: 123456789101112131415161718192021222324252627282930313233343536373839# promoters for protein-coding geneszcat gencode.v24.annotation.gtf.gz | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;&#125; $3==&quot;transcript&quot; &#123;print $1,$4-1,$5,$18,&quot;promoter&quot;,$7,$14&#125;&#x27; | tr -d &#x27;&quot;;&#x27; | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;FS=&quot;\\t&quot;&#125;&#123;if ($7==&quot;protein_coding&quot;)&#123;print $1,$2,$3,$4,$5,$6,$2,$3,&quot;102,194,165&quot;&#125;&#125;&#x27; | \\ bedtools flank -i - -g hg38.genome -l 500 -r 0 -s | \\ bedtools slop -i - -g hg38.genome -l 0 -r 500 -s &gt; promoters_500bp.bed# promoters for non-protein-coding geneszcat gencode.v24.annotation.gtf.gz | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;&#125; $3==&quot;transcript&quot; &#123;print $1,$4-1,$5,$18,&quot;promoter(NP)&quot;,$7,$14&#125;&#x27; | tr -d &#x27;&quot;;&#x27; | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;FS=&quot;\\t&quot;&#125;&#123;if ($7!=&quot;protein_coding&quot;)&#123;print $1,$2,$3,$4,$5,$6,$2,$3,&quot;102,194,165&quot;&#125;&#125;&#x27; | \\ bedtools flank -i - -g hg38.genome -l 500 -r 0 -s \\ bedtools slop -i - -g hg38.genome -l 0 -r 500 -s &gt; np_promoters_500bp.bed# intergeniczcat gencode.v24.annotation.gtf.gz | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;&#125; $3==&quot;gene&quot; &#123;print $1,$4-1,$5,$10,$16,$7&#125;&#x27; | \\ tr -d &#x27;&quot;;&#x27; | \\ bedtools slop -i - -g hg38.genome -l 500 -r 0 -s | \\ sortBed -g ../hg38.genome | \\ bedtools complement -i stdin -g ../hg38.genome | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;FS=&quot;\\t&quot;&#125;&#123;print $1,$2,$3,&quot;.&quot;,&quot;intergenic&quot;,&quot;.&quot;,$2,$3,&quot;141,160,203&quot;&#125;&#x27; &gt; intergenic_500bp.bed# exonszcat gencode.v24.annotation.gtf.gz | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;&#125; $3==&quot;exon&quot; &#123;print $1,$4-1,$5,$18,&quot;exon&quot;,$7&#125;&#x27; | \\ tr -d &#x27;&quot;;&#x27; | \\ sortBed -g ../hg38.genome | \\ mergeBed -i - -c 4,5,6 -o distinct,distinct,distinct -s | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;FS=&quot;\\t&quot;&#125;&#123;print $1,$2,$3,$4,$5,$6,$2,$3,&quot;231,138,195&quot;&#125;&#x27; &gt; exons.bed# intronszcat gencode.v24.annotation.gtf.gz | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;&#125; $3==&quot;gene&quot; &#123;print $1,$4-1,$5,$16,&quot;intron&quot;,$7&#125;&#x27; | \\ tr -d &#x27;&quot;;&#x27; | \\ sortBed -g ../hg38.genome | \\ subtractBed -a stdin -b exons.bed | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;FS=&quot;\\t&quot;&#125;&#123;print $1,$2,$3,$4,$5,$6,$2,$3,&quot;255,217,47&quot;&#125;&#x27; &gt; introns.bed# UTR, perl script from https://davetang.org/muse/2012/09/12/gencode/get_35_utr.pl gencode.v24.annotation.gtf.gz | \\ awk &#x27;BEGIN&#123;OFS=&quot;\\t&quot;;FS=&quot;\\t&quot;&#125;&#123;if ($5==&quot;3_UTR&quot;)&#123;print $1,$2,$3,$4,$5,$6,$2,$3,&quot;166,216,84&quot;&#125;else&#123;print $1,$2,$3,$4,$5,$6,$2,$3,&quot;252,141,98&quot;&#125;&#125;&#x27; &gt; utr.bedcat intergenic_500bp.bed promoters_500bp.bed np_promoters_500bp.bed utr.bed introns.bed exons.bed | sort -k1,1 -k2,2n | bgzip &gt; gencode.v24.segmented.tssflanking500b.bed.gz RefGene File name: refseq.ver109.20190125.annotation.gtf.gz (MD5 checksum: 848813de5b516e0f328046ef9c931091) Local path: &#x2F;References&#x2F;Annotations&#x2F;human&#x2F;hg38&#x2F;refseq.ver109.20190125.annotation.gtf.gz Remote backup: OSF Description: RefSeq annotation in GTF format that has been remapped to use the same set of UCSC-style sequence identifiers used in the FASTA files. The annotation is NCBI Homo sapiens Updated Annotation Release 109.20190125 from 25 January 2019. Recipe:12wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gtf.gzmv GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gtf.gz refseq.ver109.20190125.annotation.gtf.gz Other annotationsRepeat Masker File name: rmsk.bed.gz (MD5 checksum: ae12aefbef9d4f5bc7695158a67d9a55) Local path: &#x2F;References&#x2F;Annotations&#x2F;human&#x2F;hg38&#x2F;rmsk.bed.gz Remote backup: OSF Description: Repeat Masker from UCSC. The following fields were selected: genoName (Genomic sequence name) genoStart (Start in genomic sequence) genoEnd (End in genomic sequence) strand (Relative orientation + or -) repName (Name of repeat) repFamily (Family of repeat). Recipe:12345wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/rmsk.txt.gzgunzip rmsk.txt.gzgawk &#x27;OFS=&quot;\\t&quot;&#123;print $6,$7,$8,$11,$13,$10&#125;&#x27; rmsk.txt | \\ sort -k1,1 -k2,2n | \\ bgzip &gt; rmsk.bed.gz GenericSequences Primary assembly: rRNA: Human ribosomal DNA complete repeating unit, GenBank accession code: U13369.1 . AnnotationsMotif databases (MEME) File name: motif_databases.12.19.tgz (MD5 checksum: f5ffcaecc07570ee19dba20b82d7bd73) Local path: &#x2F;References&#x2F;Annotations&#x2F;human&#x2F;generic&#x2F;motif_databases.12.19.tgz Remote backup: OSF Description: Motif databases for MEME suite (updated 28 Oct 2019). Recipe:1wget http://alternate.meme-suite.org/meme-software/Databases/motifs/motif_databases.12.19.tgz Note For all fasta files, 3 standard annotations will also be generated simultaneously: .fai: index which allows for fast and random access to any sequences in the indexed fasta file. This index is generated with the following command: 1samtools faidx input.fa .genome: Table with two columns, specifying length of each chromosome. 1cut -f1,2 input.fa.fai &gt; size.genome .dict: 123java -jar picard.jar CreateSequenceDictionary \\ R=input.fa \\ O=input.dict There are two types of promoters in both gencode.v24.segmented.tssup1kb.bed and gencode.v24.segmented.tssflanking1kb.bed : Promoters for protein coding genes (denote as promoter in these files) Promoters for non-protein coding genes (denote as promoter(NP))","categories":[{"name":"posts","slug":"posts","permalink":"https://www.yaobio.com/posts/"}],"tags":[{"name":"resources","slug":"resources","permalink":"https://www.yaobio.com/tags/resources/"}]},{"title":"Inhibition of DCLK1 down-regulates PD-L1 expression through Hippo pathway in human pancreatic cancer","slug":"publications/2019-dclk1","date":"2019-12-05T14:40:14.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/inhibition-of-dclk1-down-regulates-pd-l1-expression-through-hippo-pathway-in-human-pancreatic-cancer/","link":"","permalink":"https://www.yaobio.com/publication/inhibition-of-dclk1-down-regulates-pd-l1-expression-through-hippo-pathway-in-human-pancreatic-cancer/","excerpt":"Immunotherapy is one of the most promising strategies for cancer, compared with traditional treatments. As one of the key emerging immunotherapies, anti-PD-1&#x2F;PD-L1 treatment has brought survival benefits to many advanced cancer patients. However, in pancreatic cancer, immunotherapy-based approaches have not achieved a favorable clinical effect because of mismatch repair deficiencies. Therefore, the majority of pancreatic tumors are regarded as immune-quiescent tumors and non-responsive to single-checkpoint blockade therapies. Many preclinical and clinical studies suggest that it is still important to clarify the regulatory mechanism of the PD-1&#x2F; PD-L1 pathway in pancreatic cancer. As a marker of cancer stem cells, DCLK1 has been found to play an important role in the occurrence and development of a plethora of human cancers. Recent researches have revealed that DCLK1 is closely related to EMT process of tumor cells, meanwhile, it could also be used as a biomarker in gastrointestinal tumors to predict the prognoses of patients. However, the role that DCLK1 plays in the immune regulation of tumor microenvironments remains unknown. Therefore, we sought to understand if DCLK1 could positively regulate the expression of PD-L1 in pancreatic cancer cells. Furthermore, we examined if DCLK1 highly correlated with the Hippo pathway through TCGA database analysis. We found that DCLK1 helped regulate the level of PD-L1 expression by affecting the corresponding expression level of yes-associated protein in the Hippo pathway. Collectively, our study identifies DCLK1 as an important regulator of PD-L1 expression in pancreatic tumor and highlights a central role of DCLK1 in the regulation of tumor immunity.","text":"Abstract Immunotherapy is one of the most promising strategies for cancer, compared with traditional treatments. As one of the key emerging immunotherapies, anti-PD-1&#x2F;PD-L1 treatment has brought survival benefits to many advanced cancer patients. However, in pancreatic cancer, immunotherapy-based approaches have not achieved a favorable clinical effect because of mismatch repair deficiencies. Therefore, the majority of pancreatic tumors are regarded as immune-quiescent tumors and non-responsive to single-checkpoint blockade therapies. Many preclinical and clinical studies suggest that it is still important to clarify the regulatory mechanism of the PD-1&#x2F; PD-L1 pathway in pancreatic cancer. As a marker of cancer stem cells, DCLK1 has been found to play an important role in the occurrence and development of a plethora of human cancers. Recent researches have revealed that DCLK1 is closely related to EMT process of tumor cells, meanwhile, it could also be used as a biomarker in gastrointestinal tumors to predict the prognoses of patients. However, the role that DCLK1 plays in the immune regulation of tumor microenvironments remains unknown. Therefore, we sought to understand if DCLK1 could positively regulate the expression of PD-L1 in pancreatic cancer cells. Furthermore, we examined if DCLK1 highly correlated with the Hippo pathway through TCGA database analysis. We found that DCLK1 helped regulate the level of PD-L1 expression by affecting the corresponding expression level of yes-associated protein in the Hippo pathway. Collectively, our study identifies DCLK1 as an important regulator of PD-L1 expression in pancreatic tumor and highlights a central role of DCLK1 in the regulation of tumor immunity. Accepted manuscript","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"bioSyntax","slug":"tools/biosyntax","date":"2019-11-22T11:40:40.000Z","updated":"2025-04-23T04:03:22.957Z","comments":true,"path":"/tools/biosyntax/","link":"","permalink":"https://www.yaobio.com/tools/biosyntax/","excerpt":"bioSyntax is a tool designed to simplify the inspection of biological sequences, regions, and alignments. It is compatible with multiple text editors such as vim, VS Code, sublime, less, and gedit.","text":"If you are interested in the detailed methodology, please refer to our manuscript, bioSyntax: syntax highlighting for computational biology, for more information. Summary bioSyntax is a tool designed to simplify the inspection of biological sequences, regions, and alignments. It is compatible with multiple text editors such as vim, VS Code, sublime, less, and gedit. It provides syntax highlighting for various file formats including FASTA, FASTQ, CWL, BED, GTF, PDB, PML, SAM, and VCF. This tool is a collaboration between Artem Babaian (lead developer), Anicet Ebou, Alyssa Fegen, Ho Yin Kam, German E. Novakovsky, Jasper Wong, Dylan Aïssi, and me. AvailabilitybioSyntax can be accessed through the accompany package control tools for most of the text editors. For example, if you are a Visual Studio Code user, you can install bioSyntax via Visual Studio Marketplace. Demo Source: asciinema AsciinemaPlayer.create( '/tools/biosyntax/153567.json', document.getElementById('player'), { cols: 92, rows: 33 , poster: 'npt:0:35'} );","categories":[{"name":"tools","slug":"tools","permalink":"https://www.yaobio.com/tools/"}],"tags":[]},{"title":"Extensive disruption of protein interactions by genetic variants across the allele frequency spectrum in human populations","slug":"publications/2019-interaction-variant-hpop","date":"2019-08-06T14:37:20.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/extensive-disruption-of-protein-interactions-by-genetic-variants-across-the-allele-frequency-spectrum-in-human-populations/","link":"","permalink":"https://www.yaobio.com/publication/extensive-disruption-of-protein-interactions-by-genetic-variants-across-the-allele-frequency-spectrum-in-human-populations/","excerpt":"Each human genome carries tens of thousands of coding variants. The extent to which this variation is functional and the mechanisms by which they exert their influence remains largely unexplored. To address this gap, we leverage the ExAC database of 60,706 human exomes to investigate experimentally the impact of 2009 missense single nucleotide variants (SNVs) across 2185 protein-protein interactions, generating interaction profiles for 4797 SNV-interaction pairs, of which 421 SNVs segregate at &gt; 1% allele frequency in human populations. We find that interaction-disruptive SNVs are prevalent at both rare and common allele frequencies. Furthermore, these results suggest that 10.5% of missense variants carried per individual are disruptive, a higher proportion than previously reported; this indicates that each individual’s genetic makeup may be significantly more complex than expected. Finally, we demonstrate that candidate disease-associated mutations can be identified through shared interaction perturbations between variants of interest and known disease mutations.","text":"Abstract Each human genome carries tens of thousands of coding variants. The extent to which this variation is functional and the mechanisms by which they exert their influence remains largely unexplored. To address this gap, we leverage the ExAC database of 60,706 human exomes to investigate experimentally the impact of 2009 missense single nucleotide variants (SNVs) across 2185 protein-protein interactions, generating interaction profiles for 4797 SNV-interaction pairs, of which 421 SNVs segregate at &gt; 1% allele frequency in human populations. We find that interaction-disruptive SNVs are prevalent at both rare and common allele frequencies. Furthermore, these results suggest that 10.5% of missense variants carried per individual are disruptive, a higher proportion than previously reported; this indicates that each individual’s genetic makeup may be significantly more complex than expected. Finally, we demonstrate that candidate disease-associated mutations can be identified through shared interaction perturbations between variants of interest and known disease mutations. Published manuscript","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"bioSyntax: syntax highlighting for computational biology","slug":"publications/2018-biosyntax","date":"2018-08-14T14:34:43.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/biosyntax-syntax-highlighting-for-computational-biology/","link":"","permalink":"https://www.yaobio.com/publication/biosyntax-syntax-highlighting-for-computational-biology/","excerpt":"Background: Computational biology requires the reading and comprehension of biological data files. Plain-text formats such as SAM, VCF, GTF, PDB and FASTA, often contain critical information which is obfuscated by the data structure complexity. Results: bioSyntax is a freely available suite of biological syntax highlighting packages for vim, gedit, Sublime, VSCode, and less. bioSyntax improves the legibility of low-level biological data in the bioinformatics workspace. Conclusion: bioSyntax supports computational scientists in parsing and comprehending their data efficiently and thus can accelerate research output.","text":"Abstract Background: Computational biology requires the reading and comprehension of biological data files. Plain-text formats such as SAM, VCF, GTF, PDB and FASTA, often contain critical information which is obfuscated by the data structure complexity. Results: bioSyntax is a freely available suite of biological syntax highlighting packages for vim, gedit, Sublime, VSCode, and less. bioSyntax improves the legibility of low-level biological data in the bioinformatics workspace. Conclusion: bioSyntax supports computational scientists in parsing and comprehending their data efficiently and thus can accelerate research output. Published manuscript","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"BioQueue","slug":"tools/bioqueue","date":"2017-11-22T11:40:40.000Z","updated":"2025-04-23T04:03:22.949Z","comments":true,"path":"/tools/bioqueue/","link":"","permalink":"https://www.yaobio.com/tools/bioqueue/","excerpt":"BioQueue is a researcher-facing platform preferentially to improve the efficiency and robustness of analysis in bioinformatics research. In this post, details about how BioQueue tries to achieve these two goals are explained.","text":"If you are interested in the detailed methodology, please refer to our manuscript, BioQueue: a novel pipeline framework to accelerate bioinformatics analysis, for more information. BioQueue is a researcher-facing platform preferentially to improve the efficiency and robustness of analysis in bioinformatics research. In this post, details about how BioQueue tries to achieve these two goals are explained. Speeding up analysisBioQueue organizes analysis workflows as protocols, which are composed of continuous steps. For example, to quantify the expression of genes from RNA-seq samples, we can use the workflow from (Pertea et al., 2016); in BioQueue, it can be expressed as follows: Limited by the design of software or the system resources, many currently available tools cannot fully use the system resources allotted to them or may not achieve excellent efficiency and can even generate errors (such as memory overflow) when running multiple jobs simultaneously. When running each step, BioQueue monitors the actual resources (CPU, peak memory usage, and disk usage) that a step occupies and predicts future resources usage for this step from collected data. It then uses a greedy-algorithm-based dispatcher to arrange the execution order of multiple jobs to ensure the maximum usage of resources and thus speed up the overall efficiency of analysis. For analyzing the same set of data with the identical pipeline, BioQueue can save up to 46% of the required time (boosting rates vary depending on protocols). Protecting the integrity of resultsTo protect the integrity of analysis results from human errors (like overwriting a result file by accident), BioQueue actively scans the changes on the inputs and outputs that jobs depend and produce. If files are changed after the job is finished, a color indication (red) will be added to the corresponding job cards. Another common situation is that after a specific period, you may need to rerun the same analysis for different reasons (like you want to test the reproducibility of the results). BioQueue keeps track of the versions of protocols that jobs used, and if a job is not generated with the latest protocol, another color indication (yellow) will be added to the job cards warning that rerun the job may yield inconsistent results. BioQueue also supports achieving job files and backup them to a different destination to prevent the effect of disk failure. Other handy functions Steps can be executed in different conda&#x2F;venv environments so that you can handle dependencies that different tools&#x2F;software requires with ease; BioQueue provides a registry of biosamples, and automatically associates jobs with registered samples; BioQueue has two types of account groups: worker (with read and write permission) and viewer (read only). You can create viewer accounts and share the account info with your partners, they will be able to download files, and you don’t need to worry about jobs’ safety. Useful links: Original paper GitHub repo","categories":[{"name":"tools","slug":"tools","permalink":"https://www.yaobio.com/tools/"}],"tags":[]},{"title":"Large-scale prediction of ADAR-mediated effective human A-to-I RNA editing","slug":"publications/2017-rna-editing-plus","date":"2017-08-10T14:30:33.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/large-scale-prediction-of-adar-mediated-effective-human-a-to-i-rna-editing/","link":"","permalink":"https://www.yaobio.com/publication/large-scale-prediction-of-adar-mediated-effective-human-a-to-i-rna-editing/","excerpt":"Adenosine-to-inosine (A-to-I) editing by adenosine deaminase acting on the RNA (ADAR) proteins is one of the most frequent modifications during post- and co-transcription. To facilitate the assignment of biological functions to specific editing sites, we designed an automatic online platform to annotate A-to-I RNA editing sites in pre-mRNA splicing signals, microRNAs (miRNAs) and miRNA target untranslated regions ($3^\\prime$ UTRs) from human (Homo sapiens) high-throughput sequencing data and predict their effects based on large-scale bioinformatic analysis. After analysing plenty of previously reported RNA editing events and human normal tissues RNA high-seq data, &gt;60000 potentially effective RNA editing events on functional genes were found. The RNA Editing Plus platform is available for free at https://www.rnaeditplus.org/, and we believe our platform governing multiple optimized methods will improve further studies of A-to-I-induced editing post-transcriptional regulation.","text":"Abstract Adenosine-to-inosine (A-to-I) editing by adenosine deaminase acting on the RNA (ADAR) proteins is one of the most frequent modifications during post- and co-transcription. To facilitate the assignment of biological functions to specific editing sites, we designed an automatic online platform to annotate A-to-I RNA editing sites in pre-mRNA splicing signals, microRNAs (miRNAs) and miRNA target untranslated regions ($3^\\prime$ UTRs) from human (Homo sapiens) high-throughput sequencing data and predict their effects based on large-scale bioinformatic analysis. After analysing plenty of previously reported RNA editing events and human normal tissues RNA high-seq data, &gt;60000 potentially effective RNA editing events on functional genes were found. The RNA Editing Plus platform is available for free at https://www.rnaeditplus.org/, and we believe our platform governing multiple optimized methods will improve further studies of A-to-I-induced editing post-transcriptional regulation. Accepted manuscript NotesDue to funding issue, the online platform is no longer under maintenance. Original codes are part of the supplement, and datasets stored in our database is now available at OSF.","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"BioQueue: a novel pipeline framework to accelerate bioinformatics analysis","slug":"publications/2017-bioqueue","date":"2017-06-16T14:23:26.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/bioqueue-a-novel-pipeline-framework-to-accelerate-bioinformatics-analysis/","link":"","permalink":"https://www.yaobio.com/publication/bioqueue-a-novel-pipeline-framework-to-accelerate-bioinformatics-analysis/","excerpt":"Motivation: With the rapid development of Next-Generation Sequencing, a large amount of data is now available for bioinformatics research. Meanwhile, the presence of many pipeline frameworks makes it possible to analyse these data. However, these tools concentrate mainly on their syntax and design paradigms, and dispatch jobs based on users’ experience about the resources needed by the execution of a certain step in a protocol. As a result, it is difficult for these tools to maximize the potential of computing resources, and avoid errors caused by overload, such as memory overflow. Results: Here, we have developed BioQueue, a web-based framework that contains a checkpoint before each step to automatically estimate the system resources (CPU, memory and disk) needed by the step and then dispatch jobs accordingly. BioQueue possesses a shell command-like syntax instead of implementing a new script language, which means most biologists without computer programming background can access the efficient queue system with ease.","text":"Abstract Motivation: With the rapid development of Next-Generation Sequencing, a large amount of data is now available for bioinformatics research. Meanwhile, the presence of many pipeline frameworks makes it possible to analyse these data. However, these tools concentrate mainly on their syntax and design paradigms, and dispatch jobs based on users’ experience about the resources needed by the execution of a certain step in a protocol. As a result, it is difficult for these tools to maximize the potential of computing resources, and avoid errors caused by overload, such as memory overflow. Results: Here, we have developed BioQueue, a web-based framework that contains a checkpoint before each step to automatically estimate the system resources (CPU, memory and disk) needed by the step and then dispatch jobs accordingly. BioQueue possesses a shell command-like syntax instead of implementing a new script language, which means most biologists without computer programming background can access the efficient queue system with ease. Accepted manuscript Notes BioQueue open platform is now migrated to open.bioqueue.org. In newer versions of BioQueue, we modified the syntax, and wildcards are now encapsulated in &#123;&#123;xxx&#125;&#125; instead of &#123;xxx&#125;. For instance, the parameter for hisat2 in Table 1 should now be written as -p &#123;&#123;ThreadN&#125;&#125; --dta -x &#123;&#123;HISAT2_HG38&#125;&#125; -1 &#123;&#123;InputFile:1&#125;&#125; -2 &#123;&#123;InputFile:2&#125;&#125; -S &#123;&#123;EXP&#125;&#125;.sam.","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"Circulating microRNAs: Promising Biomarkers Involved in Several Cancers and Other Diseases","slug":"publications/2016-circulating-mirna","date":"2016-11-26T14:08:37.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/publication/circulating-micrornas-promising-biomarkers-involved-in-several-cancers-and-other-diseases/","link":"","permalink":"https://www.yaobio.com/publication/circulating-micrornas-promising-biomarkers-involved-in-several-cancers-and-other-diseases/","excerpt":"Recently, many studies indicated that microRNAs (miRNAs) stably existed in various body fluids, including serum, plasma, saliva, and urine. Such miRNAs that exist in mammalian body fluids are known as circulating miRNAs, and they can transmit signals between cells and regulate intracellular gene expression. Currently, we barely understand the characteristics, sources, secretion, uptake, and functions of newly generated miRNAs. Particularly, it has been shown that certain types of circulating miRNAs can provide effective clinical data, suggesting their roles as novel biomarkers for the early detection of diseases such as cancers, cardiovascular diseases, and diabetes. Therefore, miRNAs have attracted much attention in academia for their promising applications in fundamental research and clinical diagnosis. This review summarizes some of the functional studies that have been conducted as well as the promising applications of circulating miRNAs, and we hope it will benefit other researchers in this field.","text":"Abstract Recently, many studies indicated that microRNAs (miRNAs) stably existed in various body fluids, including serum, plasma, saliva, and urine. Such miRNAs that exist in mammalian body fluids are known as circulating miRNAs, and they can transmit signals between cells and regulate intracellular gene expression. Currently, we barely understand the characteristics, sources, secretion, uptake, and functions of newly generated miRNAs. Particularly, it has been shown that certain types of circulating miRNAs can provide effective clinical data, suggesting their roles as novel biomarkers for the early detection of diseases such as cancers, cardiovascular diseases, and diabetes. Therefore, miRNAs have attracted much attention in academia for their promising applications in fundamental research and clinical diagnosis. This review summarizes some of the functional studies that have been conducted as well as the promising applications of circulating miRNAs, and we hope it will benefit other researchers in this field.","categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"}],"tags":[]},{"title":"In silico analysis frameworks","slug":"project_analysis_framework","date":"2016-04-19T17:11:09.000Z","updated":"2025-04-23T04:03:22.947Z","comments":true,"path":"/projects/in-silico-analysis-frameworks/","link":"","permalink":"https://www.yaobio.com/projects/in-silico-analysis-frameworks/","excerpt":"","text":"Active period: 2016~ It’s a critical question in bioinformatics that how we should organize and analyze data to maximize efficiency and reproducibility. Part of my research interest is to answer it. Efforts from multiple aspects have been made, and I’ll briefly walk through them in this post. Infrasturecture BioQueue: a platform to run analysis jobs with high efficiency and reproducibility Dependencies*","categories":[{"name":"projects","slug":"projects","permalink":"https://www.yaobio.com/projects/"}],"tags":[]}],"categories":[{"name":"publication","slug":"publication","permalink":"https://www.yaobio.com/publication/"},{"name":"posts","slug":"posts","permalink":"https://www.yaobio.com/posts/"},{"name":"tools","slug":"tools","permalink":"https://www.yaobio.com/tools/"},{"name":"teaching","slug":"teaching","permalink":"https://www.yaobio.com/teaching/"},{"name":"projects","slug":"projects","permalink":"https://www.yaobio.com/projects/"}],"tags":[{"name":"technical_docs","slug":"technical-docs","permalink":"https://www.yaobio.com/tags/technical-docs/"},{"name":"debug","slug":"debug","permalink":"https://www.yaobio.com/tags/debug/"},{"name":"orientation","slug":"orientation","permalink":"https://www.yaobio.com/tags/orientation/"},{"name":"best_practice","slug":"best-practice","permalink":"https://www.yaobio.com/tags/best-practice/"},{"name":"resources","slug":"resources","permalink":"https://www.yaobio.com/tags/resources/"}]}